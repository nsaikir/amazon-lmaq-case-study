a sample scenario and code for "Run and automate ETL jobs for processing and integrating large scale datasets in AWS" relevant to the Business Analyst, Last Mile Analytics and Quality role.

Sample: Run and Automate ETL Jobs in AWS
Scenario:
Suppose the team receives daily map and routing data files in Amazon S3. The requirement is to automate an ETL (Extract, Transform, Load) job that:

Extracts new data from S3,
Cleans and transforms the data using AWS Glue (with Python),
Loads the cleaned data into an Amazon Redshift table for analytics.
Step 1: Define the ETL Job in AWS Glue (Python Script Example)

glue_etl_job.py

import sys
from awsglue.transforms import *
from awsglue.utils import getResolvedOptions
from pyspark.context import SparkContext
from awsglue.context import GlueContext
from awsglue.job import Job

# Arguments for job name, source and target
args = getResolvedOptions(sys.argv, ['JOB_NAME', 'SOURCE_S3_PATH', 'TARGET_REDSHIFT_TABLE', 'REDSHIFT_CONNECTION'])

sc = SparkContext()
glueContext = GlueContext(sc)
spark = glueContext.spark_session
job = Job(glueContext)
job.init(args['JOB_NAME'], args)

# Extract data from S3
datasource = glueContext.create_dynamic_frame.from_options(
    connection_type="s3",
    connection_options={"paths": [args['SOURCE_S3_PATH']]},
    format="csv",
    format_options={"withHeader": True}
)

# Transform: Clean data (sample: remove rows with nulls in 'route_id')
cleaned_data = datasource.drop_nulls(['route_id'])

# (Optional) Additional transformations, e.g., data type casting, filtering, joins

# Load data into Amazon Redshift
glueContext.write_dynamic_frame.from_jdbc_conf(
    frame=cleaned_data,
    catalog_connection=args['REDSHIFT_CONNECTION'],
    connection_options={"dbtable": args['TARGET_REDSHIFT_TABLE'], "database": "lmaq_analytics"},
    redshift_tmp_dir="s3://your-glue-temp-dir/"
)

job.commit()

Step 2: Schedule the Job with AWS Glue or AWS Step Functions
Use AWS Glue Job Scheduler or Step Functions to run this job daily or on data arrival (event-based with S3 triggers).

Step 3: Monitor and Validate
Set up CloudWatch alerts for job failures or anomalies.
Add data quality checks in the script to validate row counts or key metrics.

sample code and configuration files for automating an ETL process using three popular AWS tools: AWS Lambda, AWS Step Functions, and AWS Data Pipeline. These examples focus on moving data from S3, transforming it, and loading it to Redshift—matching your Last Mile Analytics and Quality scenario.

1. AWS Lambda ETL Example
This Lambda function reads a CSV file from S3, performs a simple transformation, and loads data into Redshift using the psycopg2 library.

import boto3
import csv
import psycopg2
import os

S3_BUCKET = os.environ['S3_BUCKET']
REDSHIFT_HOST = os.environ['REDSHIFT_HOST']
REDSHIFT_DB = os.environ['REDSHIFT_DB']
REDSHIFT_USER = os.environ['REDSHIFT_USER']
REDSHIFT_PASSWORD = os.environ['REDSHIFT_PASSWORD']
REDSHIFT_PORT = 5439

def lambda_handler(event, context):
    s3 = boto3.client('s3')
    key = event['Records'][0]['s3']['object']['key']
    response = s3.get_object(Bucket=S3_BUCKET, Key=key)
    rows = []
    for line in response['Body'].read().decode('utf-8').splitlines():
        row = next(csv.reader([line]))
        # Example transformation: uppercase all columns
        rows.append([col.upper() for col in row])

    # Connect to Redshift and insert data
    conn = psycopg2.connect(
        host=REDSHIFT_HOST,
        dbname=REDSHIFT_DB,
        user=REDSHIFT_USER,
        password=REDSHIFT_PASSWORD,
        port=REDSHIFT_PORT
    )
    cur = conn.cursor()
    for row in rows:
        cur.execute("INSERT INTO your_table VALUES (%s, %s, %s)", row)
    conn.commit()
    cur.close()
    conn.close()
    return {'statusCode': 200, 'body': f'Processed {len(rows)} rows'}

Trigger: S3 PUT event
IAM: Lambda needs permissions for S3:GetObject and Redshift:Connect.

2. AWS Step Functions ETL Orchestration
Here’s a JSON definition for a Step Functions state machine that orchestrates Glue ETL, with error handling and notifications.

{
  "Comment": "ETL workflow for Last Mile Analytics",
  "StartAt": "Start Glue ETL Job",
  "States": {
    "Start Glue ETL Job": {
      "Type": "Task",
      "Resource": "arn:aws:states:::glue:startJobRun.sync",
      "Parameters": {
        "JobName": "lmaq-glue-etl-job"
      },
      "Next": "Success Notification",
      "Catch": [
        {
          "ErrorEquals": ["States.ALL"],
          "Next": "Failure Notification"
        }
      ]
    },
    "Success Notification": {
      "Type": "Task",
      "Resource": "arn:aws:states:::sns:publish",
      "Parameters": {
        "TopicArn": "arn:aws:sns:us-east-1:123456789012:lmaq-notifications",
        "Message": "ETL Job succeeded."
      },
      "End": true
    },
    "Failure Notification": {
      "Type": "Task",
      "Resource": "arn:aws:states:::sns:publish",
      "Parameters": {
        "TopicArn": "arn:aws:sns:us-east-1:123456789012:lmaq-notifications",
        "Message": "ETL Job failed."
      },
      "End": true
    }
  }
}

How to use:
Create a Glue ETL job named lmaq-glue-etl-job.
Replace ARNs with your own resources.

3. AWS Data Pipeline ETL Example
A simple Data Pipeline JSON definition to move data from S3 to Redshift nightly.

{
  "objects": [
    {
      "id": "Default",
      "name": "Default",
      "fields": [
        {"key": "scheduleType", "stringValue": "cron"}
      ]
    },
    {
      "id": "Schedule",
      "type": "Schedule",
      "fields": [
        {"key": "startDateTime", "stringValue": "2025-06-10T00:00:00"},
        {"key": "period", "stringValue": "1 day"}
      ]
    },
    {
      "id": "S3Input",
      "type": "S3DataNode",
      "fields": [
        {"key": "directoryPath", "stringValue": "s3://your-bucket/input/"}
      ]
    },
    {
      "id": "RedshiftTable",
      "type": "RedshiftDataNode",
      "fields": [
        {"key": "tableName", "stringValue": "your_table"},
        {"key": "database", "stringValue": "your_db"}
      ]
    },
    {
      "id": "RedshiftCopyActivity",
      "type": "RedshiftCopyActivity",
      "fields": [
        {"key": "input", "refValue": "S3Input"},
        {"key": "output", "refValue": "RedshiftTable"},
        {"key": "schedule", "refValue": "Schedule"},
        {"key": "runsOn", "refValue": "Ec2Resource"},
        {"key": "copyOptions", "stringValue": "csv"}
      ]
    },
    {
      "id": "Ec2Resource",
      "type": "Ec2Resource",
      "fields": [
        {"key": "instanceType", "stringValue": "m5.large"}
      ]
    }
  ]
}

How to use:
Upload this to AWS Data Pipeline as a pipeline definition.
Set S3 and Redshift details as per your environment.
